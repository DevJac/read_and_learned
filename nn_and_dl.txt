An online book found at: http://neuralnetworksanddeeplearning.com/

---------------------
Thoughts
---------------------

Q: Do neural networks reinforce correct answers, or only wrong answers?
A: They attempt to correct errors. There is almost always some error, so they
   would reinforce correct answers a small amount. They punish incorrect answers
   much more generally. This is desirable behavior.

Q: Why are the weights updated in proportion to the incoming activations?
A: ???

---------------------
Chapter 1 & 2
---------------------

I implemented a working neural network in about 60 lines of Julia. It is trained
by backpropagation as described in chapter 2.

Julia supports automatic differentiation of functions using a variety of
methods. I tried the Calculus and ForwardDiff packages. Both perform very
similarly.

`sigmoid(n) = 1 / (1 + e^(-n))` is a common activation function. It is an
s-shaped function from 0 to 1. sigmoid(0) = 5

Any computation can be build from NAND gates, and a neuron in a neural network
can simulate a NAND gate, so a properly trained neural network can perform
any computation.

"In the early days of AI research people hoped that the effort to build an AI
 would also help us understand the principles behind intelligence and, maybe,
 the functioning of the human brain. But perhaps the outcome will be that we end
 up understanding neither the brain nor how artificial intelligence works!"
    -- Michael Nielsen

Here's my rough summary of backpropagation:

  1) Feed some data forward through the network. Take note of how the output differs
     from the desired result.
  2) Set the error of the output layer to be equal to the difference between
     the desired result and the actual result.
  3) Backpropagate the error. The error of a layer is equal to the weights times
     the error of the following layer, and then times the gradient (slope) of the
     activation function during the forward pass.
  4) Update the biases in direct proportion to the error.
  5) Update the biases in proportion to the error time the incoming activation
     values.

Conditions that will cause slow learning:
  - The output is close to the desired result. In this case you don't want to
    change the network much, because it's already behaving well.
  - A saturated activation function. Because the error is multiplied by the
    gradient of the activation function. If the activation function has a low
    slope (at the point used during the forward pass), it will cause the error
    to be multiplied by a small number and thus learning will be slow.
  - The error applied to weights is multiplied by the incoming activations. Thus
    if an incoming activation is small, the weight will be updated only a little.

-----------------------
Chapter 3
-----------------------

The cross-entropy cost function can speed up training. In practice, you just
remove the activation function gradient from backpropagation (step 3). This way
learning doesn't slow down at the flat portions of the activation function,
which is usually "s" shaped.

"Softmax" is useful at the output layer. In a softmax layer all the activations
sum to 1, meaning the activations can be interpreted as percentages. Softmax
is an activation function.

When performing softmax, first calculate the weighted inputs to the activation
function (z), then the activation function has the form:
  a = (e^z) / (sum_for_all_zs_in_layer(e^z))
where a is the activation values going out of the softmax layer, and z is the
weighted values before the activation function (in this case, softmax)
is applied.

Beware of overfitting. Overfitting occurs when performance on the training set
is improving, but the performance on the test set is falling.

A common problem with neural networks is plateauing, where performance stops
improving for a time, and then resumes.

Regularizing weights can be done by multiplying all weights by a number slightly
less than 1, thus pulling all weights towards zero. The weights will tend to be
close to zero, although if it's highly beneficial to have a large weight, then
it will overpower the pull towards zero. This can be described as
"weight decay." This is also known as L2 regularization.

L1 vs L2 regularization:
  L1 regularization adds a constant amount to each weight to pull them to zero.
  L2 regularization multiplies by a number slightly less than 1, causing the
  weights to decay towards zero.

  Note that small weights can be multiplied forever and never reach exactly zero
  in L2 regularization. Thus, L2 regularization allows many non-zero weights
  (or parameters). L2 has it's largest effect on large weights.
  L2 regularization has almost no effect on small weights, which are already
  near zero.

  L1 regularization adds or subtracts a constant from all weights. Small
  weights has the constant addition as large weights. Thus, small weights can
  easily be "whipped out" and made to be zero. While large weights can grow
  with less restriction as they become large.
  L1 regularization tends to result in many weights/parameters being equal to
  zero.

  L1 regularization can be good for feature selection, but L2 regularization
  usually results in better performance overall.

Dropout is a regularization method in which half the hidden neurons are ignored
during a feed-forward / backprop cycle. The neurons to be ignored are chosen
randomly, and they are not used in the feed-forward calculation, and their
parameters are not updated by the backpropagation. When all neurons are used
in production, the weights coming out of the hidden layers have to be scaled
to account for all the neurons being enabled, which is a different than the
training conditions. Intuitively, the result of this training approach is that
each neuron learns to be more self-sufficient, because it cannot depend on
other neurons always being there, because those other neurons might be disabled.

Another way to regularize it to artificially expand the training set by applying
transformations to the original data that do not change the outcome. For example,
to expand the MNIST dataset of hand-written digits you could slightly rotate the
digits, or scale them in slight ways. A slightly rotated or skewed "5" is still
a "5". Thus, you can artificially generate new training data, and the larger
training set will reduce overfitting.

"It sometimes happens that algorithm A will outperform algorithm B with one set
 of training data, while algorithm B will outperform algorithm A with a
 different set of training data.... Many papers focus on finding new tricks to
 wring out improved performance on standard benchmark data sets. 'Our whiz-bang
 technique gave us an improvement of X percent on standard benchmark Y' is a
 canonical form of research claim.... It's entirely possible that the
 improvement due to the whiz-bang technique would disappear on a larger data
 set. In other words, the purported improvement might be just an accident of
 history. The message to take away, especially in practical applications, is
 that what we want is both better algorithms and better training data.
 It's fine to look for better algorithms, but make sure you're not focusing on
 better algorithms to the exclusion of easy wins getting more or better
 training data."
    -- Michael Nielsen

If the starting weights are to large in magnitude, the network will start
saturated and take longer to train. The book suggests initializing weights with
a Gaussian distribution centered and zero with a standard deviation of
1 / sqrt(n), where n is the number of connections coming into the neuron.

When approaching a new machine learning task, and learning is proving to be
difficult to achieve. First, simplify the problem as much as possible and make
the feedback loop short. Once you achieve a little bit of learning, turn the
hyper-parameters to find further improvements. There are some automated ways to
optimize hyper-parameters, e.g.: grid search, Bayesian optimization
See:
  http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf

"There is a saying common among writers that books are never finished, only
 abandoned. The same is also true of neural network optimization: the space of
 hyper-parameters is so large that one never really finishes optimizing, one
 only abandons the network to posterity."
    -- Michael Nielsen

Momentum based gradient decent is a "Hessian technique" that often converges
more quickly than Stochastic Gradient Descent. In this method the gradient
of the cost function does not direct determine the change of weights or biases,
but instead influences a velocity vector. This is sort of like simulating a
physical ball rolling down hill with momentum. The is a hyper-parameter that
controls the amount of damping or "friction" on the velocity vector. To
implement this you would need an additional matrix, the same size as the weights
matrix, to store the velocity of the weights. The gradient of the cost function
updates the velocities, which then update the weights. Remember to implement
a damping (like friction) on the velocities, so they'll eventually settle down,
and to use a learning rate scalar.

"tanh" is an alternative to the regular sigmoid function. tanh is a scaled
sigmoid function. It may learn faster than sigmoid.

There are also "rectified linear neurons", which use the following activation
function:
  max(0, w*x+b)
These can also compute any function with the correct weights and biases. This
activation function has proven useful in image classification tasks.

In neural network research we often have to rely on empirical evidence, because
our mathematical understanding of neural networks is limited.

------------------
Chapter 4
------------------
