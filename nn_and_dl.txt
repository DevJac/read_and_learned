An online book found at: http://neuralnetworksanddeeplearning.com/

---------------------
Thoughts
---------------------

Q: Do neural networks reinforce correct answers, or only wrong answers?
A: They attempt to correct errors. There is almost always some error, so they
   would reinforce correct answers a small amount. They punish incorrect answers
   much more generally. This is desirable behavior.

Q: Why are the weights updated in proportion to the incoming activations?
A: ???

---------------------
Chapter 1 & 2
---------------------

I implemented a working neural network in about 60 lines of Julia. It is trained
by backpropagation as described in chapter 2.

Julia supports automatic differentiation of functions using a variety of
methods. I tried the Calculus and ForwardDiff packages. Both perform very
similarly.

`sigmoid(n) = 1 / (1 + e^(-n))` is a common activation function. It is an
s-shaped function from 0 to 1. sigmoid(0) = 5

Any computation can be build from NAND gates, and a neuron in a neural network
can simulate a NAND gate, so a properly trained neural network can perform
any computation.

"In the early days of AI research people hoped that the effort to build an AI
 would also help us understand the principles behind intelligence and, maybe,
 the functioning of the human brain. But perhaps the outcome will be that we end
 up understanding neither the brain nor how artificial intelligence works!"
    -- Michael Nielsen

Here's my rough summary of backpropagation:

  1) Feed some data forward through the network. Take note of how the output differs
     from the desired result.
  2) Set the error of the output layer to be equal to the difference between
     the desired result and the actual result.
  3) Backpropagate the error. The error of a layer is equal to the weights times
     the error of the following layer, and then times the gradient (slope) of the
     activation function during the forward pass.
  4) Update the biases in direct proportion to the error.
  5) Update the biases in proportion to the error time the incoming activation
     values.

Conditions that will cause slow learning:
  - The output is close to the desired result. In this case you don't want to
    change the network much, because it's already behaving well.
  - A saturated activation function. Because the error is multiplied by the
    gradient of the activation function. If the activation function has a low
    slope (at the point used during the forward pass), it will cause the error
    to be multiplied by a small number and thus learning will be slow.
  - The error applied to weights is multiplied by the incoming activations. Thus
    if an incoming activation is small, the weight will be updated only a little.

-----------------------
Chapter 3
-----------------------

The cross-entropy cost function can speed up training. In practice, you just
remove the activation function gradient from backpropagation (step 3). This way
learning doesn't slow down at the flat portions of the activation function,
which is usually "s" shaped.

"Softmax" is useful at the output layer. In a softmax layer all the activations
sum to 1, meaning the activations can be interpreted as percentages. Softmax
is an activation function.

When performing softmax, first calculate the weighted inputs to the activation
function (z), then the activation function has the form:
  a = (e^z) / (sum_for_all_zs_in_layer(e^z))
where a is the activation values going out of the softmax layer, and z is the
weighted values before the activation function (in this case, softmax)
is applied.

Beware of overfitting. Overfitting occurs when performance on the training set
is improving, but the performance on the test set is falling.

A common problem with neural networks is plateauing, where performance stops
improving for a time, and then resumes.

Regularizing weights can be done by multiplying all weights by a number slightly
less than 1, thus pulling all weights towards zero. The weights will tend to be
close to zero, although if it's highly beneficial to have a large weight, then
it will overpower the pull towards zero. This can be described as
"weight decay." This is also known as L2 regularization.

L1 vs L2 regularization:
  L1 regularization adds a constant amount to each weight to pull them to zero.
  L2 regularization multiplies by a number slightly less than 1, causing the
  weights to decay towards zero.

  Note that small weights can be multiplied forever and never reach exactly zero
  in L2 regularization. Thus, L2 regularization allows many non-zero weights
  (or parameters). L2 has it's largest effect on large weights.
  L2 regularization has almost no effect on small weights, which are already
  near zero.

  L1 regularization adds or subtracts a constant from all weights. Small
  weights has the constant addition as large weights. Thus, small weights can
  easily be "whipped out" and made to be zero. While large weights can grow
  with less restriction as they become large.
  L1 regularization tends to result in many weights/parameters being equal to
  zero.

  L1 regularization can be good for feature selection, but L2 regularization
  usually results in better performance overall.

Dropout is a regularization method in which half the hidden neurons are ignored
during a feed-forward / backprop cycle. The neurons to be ignored are chosen
randomly, and they are not used in the feed-forward calculation, and their
parameters are not updated by the backpropagation. When all neurons are used
in production, the weights coming out of the hidden layers have to be scaled
to account for all the neurons being enabled, which is a different than the
training conditions. Intuitively, the result of this training approach is that
each neuron learns to be more self-sufficient, because it cannot depend on
other neurons always being there, because those other neurons might be disabled.

Another way to regularize it to artificially expand the training set by applying
transformations to the original data that do not change the outcome. For example,
to expand the MNIST dataset of hand-written digits you could slightly rotate the
digits, or scale them in slight ways. A slightly rotated or skewed "5" is still
a "5". Thus, you can artificially generate new training data, and the larger
training set will reduce overfitting.

"It sometimes happens that algorithm A will outperform algorithm B with one set
 of training data, while algorithm B will outperform algorithm A with a
 different set of training data.... Many papers focus on finding new tricks to
 wring out improved performance on standard benchmark data sets. 'Our whiz-bang
 technique gave us an improvement of X percent on standard benchmark Y' is a
 canonical form of research claim.... It's entirely possible that the
 improvement due to the whiz-bang technique would disappear on a larger data
 set. In other words, the purported improvement might be just an accident of
 history. The message to take away, especially in practical applications, is
 that what we want is both better algorithms and better training data.
 It's fine to look for better algorithms, but make sure you're not focusing on
 better algorithms to the exclusion of easy wins getting more or better
 training data."
    -- Michael Nielsen

If the starting weights are to large in magnitude, the network will start
saturated and take longer to train. The book suggests initializing weights with
a Gaussian distribution centered and zero with a standard deviation of
1 / sqrt(n), where n is the number of connections coming into the neuron.

When approaching a new machine learning task, and learning is proving to be
difficult to achieve. First, simplify the problem as much as possible and make
the feedback loop short. Once you achieve a little bit of learning, turn the
hyper-parameters to find further improvements. There are some automated ways to
optimize hyper-parameters, e.g.: grid search, Bayesian optimization
See:
  http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf

"There is a saying common among writers that books are never finished, only
 abandoned. The same is also true of neural network optimization: the space of
 hyper-parameters is so large that one never really finishes optimizing, one
 only abandons the network to posterity."
    -- Michael Nielsen

Momentum based gradient decent is a "Hessian technique" that often converges
more quickly than Stochastic Gradient Descent. In this method the gradient
of the cost function does not direct determine the change of weights or biases,
but instead influences a velocity vector. This is sort of like simulating a
physical ball rolling down hill with momentum. The is a hyper-parameter that
controls the amount of damping or "friction" on the velocity vector. To
implement this you would need an additional matrix, the same size as the weights
matrix, to store the velocity of the weights. The gradient of the cost function
updates the velocities, which then update the weights. Remember to implement
a damping (like friction) on the velocities, so they'll eventually settle down,
and to use a learning rate scalar.

"tanh" is an alternative to the regular sigmoid function. tanh is a scaled
sigmoid function. It may learn faster than sigmoid.

There are also "rectified linear neurons", which use the following activation
function:
  max(0, w*x+b)
These can also compute any function with the correct weights and biases. This
activation function has proven useful in image classification tasks.

In neural network research we often have to rely on empirical evidence, because
our mathematical understanding of neural networks is limited.

------------------
Chapter 4
------------------

Neural networks and universal and can approximate any [continuous] function. So
long as the neural network is sufficiently large, and is trained appropriately.

Neural networks must using an appropriate activation function (like sigmoid) in
order to be universal.

----------------------------
Chapter 5
----------------------------

In deep learning layers often get "stuck" during training and learn slowly.

Early layers tend to learn slower than later layers.

The gradients are unstable through several layers of back propagation, and can
shrink or grow exponentially.

-----------------------------
Chapter 6
-----------------------------

A convolutional network is often used for image recognition tasks. It connects
a small grid from the input to a single neuron in the hidden layer.

For example, if there is a 5x5 grid, and a 40x40 input image, then the first
hidden layer will be 36x36, if the grid "stride" is 1. Each of the 36 neurons
in the hidden layer will use the same 5x5 weights and bias. This essentially
means each of the neurons in the hidden layer detect the same "feature" from
their corresponding grid in the input image.

A convolutional layer is sometimes called a "feature map" and it's weights and
biases are called "shared weights and biases". The weights can biases form
a "kernel" or "filter".

A convolutional layer can have dozens of "feature maps", forming a "layer of
many layers".

A "pooling layer" condenses information from another layer. A 24x24 layer might
be followed by a 12x12 pooling layer for example. Pooling layers are often used
after a convolutional layer. Pooling does not use learned weights and biases,
but instead a simple functions like "max" or "average". For example, a pooling
layer might have a neuron that represents the maximum value from a 2x2 section
in the preceding layer.

Convolutional layers have fewer weights and biases (parameters) than a fully
connect layer of similar size. Even with several "feature maps".

To make a deep network work:
1) Reduce the number of parameters (weights and biases)
2) Reduce overfitting
3) Use better activation functions
4) Use more CPU time (or a bigger CPU / GPU)

"""
 I've occasionally heard people adopt a deeper-than-thou attitude, holding that
 if you're not keeping-up-with-the-Joneses in terms of number of hidden layers,
 then you're not really doing deep learning. I'm not sympathetic to this attitude,
 in part because it makes the definition of deep learning into something which
 depends upon the result-of-the-moment. The real breakthrough in deep learning
 was to realize that it's practical to go beyond the shallow 1- and 2-hidden
 layer networks that dominated work until the mid-2000s. That really was a
 significant breakthrough, opening up the exploration of much more expressive
 models. But beyond that, the number of layers is not of primary fundamental
 interest. Rather, the use of deeper networks is a tool to use to help achieve
 other goals - like better classification accuracies.
 """

 "Adversarial images" are images which cannot be classified correctly by a
 deep network, even they they are almost exactly the same as an image that
 can be correctly classified. The two images are indistinguishable to a human,
 but one is slightly different, only by a few pixels, and a deep network can
 classify one correctly, but not the other. This is one example that shows we
 have a lot to learn about neural networks still.

 Other topics:
  recurrent neural networks
  Boltzmann machines
  generative models
  transfer learning
  reinforcement learning

"Recurrent neural networks" (RNNs) are NNs that have a "memory". Their output is not
dependent only on the current input, but on all past input as well. They have
been useful in speech recognition and in making "neural turning machines".

"Long short term memory units" (LSTMs) are a varient of RNNs.

"Deep belief networks" (DBFs) can be run backwards. For example, you could
train a DBF on the handwritten digits, and then play it backwards to produce
what looks like a hand written digit. They can also be used for unsupervised
learning. They have not performed well on benchmarks, and thus are not getting
a lot of attention currently, but are likely to be important in the long run.

New vocabulary word: "insipid" - lacking flavor or distinction

"Deep learning" is more about feature extraction in a hierarchy rather than
neural networks specifically. You could have deep learning that does not
involve neural networks.

Conway's law:
    Any organization that designs a system... will inevitably produce a design
    whose structure is a copy of the organization's communication structure.

"""
AI, like controlled fusion and a few other technologies, has been 10 years away
for 60 plus years. On the flipside, what we definitely do have in deep learning
is a powerful technique whose limits have not yet been found, and many wide-open
fundamental problems. That's an exciting creative opportunity.
"""
