#+STARTUP: entitiespretty
* Notation Drills
** Notation 1                                                        :drill:
What does this notation mean?

argmax_a f(a)
*** Answer
a value of a at which f(a) takes its maximal value
** Notation 2                                                        :drill:
What does this notation mean?

q_*(a)
*** Answer
true value of action a
** Notation 3                                                        :drill:
What does this notation mean?

Q_t(a)
*** Answer
estimate at time t of q_*(a)
** Notation 4                                                        :drill:
What does this notation mean?

A_t
*** Answer
action at time t
* Chapter 1 - The Reinforcement Learning Problem
** Notes
*** Elements of Reinforcement Learning
**** policy
"A policy defines the learning agent’s way of behaving at a given time. Roughly
speaking, a policy is a mapping from perceived states of the environment to actions to
be taken when in those states."
**** reward signal
"A reward signal defines the goal in a reinforcement learning problem. On each time
step, the environment sends to the reinforcement learning agent a single number, a
reward."
**** value function
"Whereas the reward signal indicates what is good in an immediate sense, a value
function specifies what is good in the long run. Roughly speaking, the value of
a state is the total amount of reward an agent can expect to accumulate over the
future, starting from that state."
** Drills
*** Elements of Reinforcement Learning                              :drill:
What are the 3 basic components of a reinforcement learning system?
**** Answer
- policy
- reward signal
- value function
* Chapter 2 - Multi-arm Bandits
** Drills
*** Formula 1                                                       :drill:
What is the formula for exponential, recency-weighted average?
**** Answer
Q_{n+1} = Q_n + \alpha(R_n - Q_n)
*** Formula 2                                                       :drill:
What is the formula for upper confidence bound?
**** Answer
A_t = argmax_a [ Q_t(a) + c * sqrt( log t / N_t(a) ) ]

Where N_t(a) is the number of times a has been chosen.
*** Formula 3                                                       :drill:
What is the formula for the gradient bandit algorithm?
**** Answer
Pr{A_t = a} = \pi_t(a) = e^{H(a)} for action a, divided by e^{H(a)} for all actions.

The update rules are:

H_{t+1}(a) = H_t(a) + \alpha (R_t - avg(R)) (1 - \pi_t(a))    for the action a that was chosen
H_{t+1}(a) = H_t(a) - \alpha (R_t - avg(R)) \pi_t(a)          for all other actions (the actions that were not chosen)

Where avg(R) is the average of all rewards received, including the current time step.
* Chapter 3 - Finite Markov Decision Processes
** Drills
*** Markov property                                                 :drill:
What is the Markov property?
**** Answer
A state signal that summarizes past sensations compactly,
yet in such a way that all relevant information is retained.

This normally requires more than the immediate sensations,
but never more than the complete history of all past sensations. A state signal
that succeeds in retaining all relevant information is said to be Markov, or to
have the Markov property.

For example, a checkers position—the current configuration of all the pieces on
the board—would serve as a Markov state.
*** Discounted return                                               :drill:
What is the formula for discounted return?
**** Answer
G_t = R_{t+1} + \gamma{} R_{t+2} + \gamma{}^2 R_{t+3} + ...

Where \gamma is a discount rate between 0 and 1.
