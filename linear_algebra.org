#+STARTUP: latexpreview
* What are vectors?
There are a few different interpretations of what a vector is:

- a magnitude and a direction
- a tuple of numbers that form a type
- anything where there is a sensible definition of adding two vectors and multiplying by a scalar
** Types vs Vectors
An xy-coordinate and a tuple of ~(age, weight)~ are essentially the same. The operations that work on one will also work on the other.
** Tuple/point notation vs Vector notation
Where we might write a tuple or a vector as ~(1, 2)~ traditionally, we would write a vertical column matrix in linear algebra:

\begin{equation*}
\begin{bmatrix}
1 \\
2
\end{bmatrix}
\end{equation*}
** Vector / Matrix size notation
\begin{bmatrix}
1 \\
2
\end{bmatrix}

Above is a 2 x 1 matrix: 2 rows x 1 column.

Or you could say "length of vectors x number of vectors"; and since vector length is most basic, it comes first in the notation. This is how I remember the order of the notation.
* Basic vector operations
** Vector addition
\begin{equation*}
\begin{bmatrix}
1 \\
2
\end{bmatrix}
+
\begin{bmatrix}
3 \\
4
\end{bmatrix}
=
\begin{bmatrix}
1 + 3 \\
2 + 4
\end{bmatrix}
=
\begin{bmatrix}
4 \\
6
\end{bmatrix}
\end{equation*}

A vector must contain values with units (types) that can be combined (added) in a sensible way.
** Vector scalar multiplication
\begin{equation*}
3
\begin{bmatrix}
1 \\
2
\end{bmatrix}
=
\begin{bmatrix}
3 \times 1 \\
3 \times 2
\end{bmatrix}
=
\begin{bmatrix}
3 \\
6
\end{bmatrix}
\end{equation*}
** Scaling unit vectors
The values in a column matrix scale "units" or "basis vectors". Notice the basic operations of addition and scalar multiplication in this single simple matrix.

\begin{equation*}
\begin{bmatrix}
1 \\
2
\end{bmatrix}
=
\begin{bmatrix}
1 \times \hat{\imath} \\
2 \times \hat{\jmath}
\end{bmatrix}
=
1 \times \hat{\imath} + 2 \times \hat{\jmath}
\end{equation*}
** Linear combinations
\begin{equation*}
a \times \hat{\imath} + b \times \hat{\jmath}
\end{equation*}

Above is a "linear combination of $\hat{\imath}$ and $\hat{\jmath}$". Notice, again, the basic operations of addition and scalar multiplication.
* Spans
The "span" of a set of vectors is the set of all possible linear combinations of those vectors.

"Basis vectors" can be though of as your building blocks. You can add the blocks, and you can scale the blocks. The "span" is the set of all vectors you could possibly build from your basis vectors using only addition and scalar multiplication.
** Linearly independent vectors
A set of vectors is said to be "linearly independent" if no vector can be expressed as a linear combination of the other vectors.

In other words, each vector brings something new (unique) to the table. Removing any of the vectors would reduce the span of the set.
** "Column space"
A set of vectors has a "span", a transformation matrix has a "column space", but the twho terms mean basically the same thing.
* Matrix multiplication
\begin{equation}
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
=
x
\begin{bmatrix}
a \\
c
\end{bmatrix}
+
y
\begin{bmatrix}
b \\
d
\end{bmatrix}
=
\begin{bmatrix}
ax + by \\
cx + dy
\end{bmatrix}
\end{equation}

This is possibly the most important equation in linear algebra.

To understand it, remember that:

\begin{equation*}
\begin{bmatrix}
x \\
y
\end{bmatrix}
=
x \times \hat{\imath} + y \times \hat{\jmath}
=
x \hat{\imath} + y \hat{\jmath}
=
x
\begin{bmatrix}
1 \\
0
\end{bmatrix}
+
y
\begin{bmatrix}
0 \\
1
\end{bmatrix}
\end{equation*}

Because of the way matrix multiplication is defined, the above matrix multiplication will change the basis vectors such that:

\begin{equation*}
\hat{\imath}
\text{ becomes }
\begin{bmatrix}
a \\
c
\end{bmatrix}
\text{ and }
\hat{\jmath}
\text{ becomes }
\begin{bmatrix}
b \\
d
\end{bmatrix}
\end{equation*}

And thus we arive back at the original formula, we have gone full circle.
** Order of matrices in matrix multiplication
The first matrix contains the new basis vectors. The order implies that a new basis is being applied to the second matrix.

This is similar to function application notation. The thing defining the transformation comes first.
* A matrix represents a linear transformation of space
All matrices represent linear transformations of space.
** Inverse transformations
The inverse of a transformation is denoted by "raising it to the power of -1".

Composing any transformation with its inverse will give you the identity transformation. But not all transformations will have an inverse. If you "squish" all of space into a single point, there is no reversing that.

If a transformation does have an inverse, it has a single unique inverse. In other words, a transformation will have either 0 or 1 inverses.
** Rank
The "rank" of a transformation is the number of dimensions in its output.

The "rank" is also the number of linearly independent columns in the transformation matrix.

If the rank of a matrix is equal to the number of columns in the matrix, it is called "full rank".
** TODO What about non-square matrices?
What does it mean for a non-square matrix to be the basis in matrix multiplication?
* Duality
A matrix can simultaneously represent data-points and a transformation.

Composing two transformation matrices will give a third matrix that is their composition.

Matrix multiplication can be used to both compose transformations and apply transformations.
** TODO Dot product duality?
Notice that ~[a b]~ appears on both sides of the dot product equation. In this case a 1x2 matrix roughly behaves the same as a 2x1 column vector. Are there other cases like this? Maybe this is simply a result of the way the dot production operation is defined? Remember that one side of the equation is doing matrix multiplication and the other is doing a dot product, they are not the same operation.
* Determinant
The determinant is the "scaling factor" of a transformation.

For example, a determinant of 2 would mean that any area inside the original space would be doubled by the transformation.
** Negative determinants
Determinants may be negative, which means the transformation will invert the space.

For example, a determinant of -0.5 would mean that the space will be inverted, and any area inside the original space will be halved.
** Zero determinants
A determinant of 0 means the original space will be transformed ("squished") into a lower dimension.

It also means the vectors in the transformation matrix are linearly dependent (at least one of them is contributing nothing new to the span).
* Dot product
\begin{equation*}
\begin{bmatrix}
a & b
\end{bmatrix}
\begin{bmatrix}
c \\
d
\end{bmatrix}
=
\begin{bmatrix}
a \\
b
\end{bmatrix}
\cdot
\begin{bmatrix}
c \\
d
\end{bmatrix}
\end{equation*}

The dot product is a projection into one dimension (a scalar).

The dot product is commutative, the order of the operands does not effect the result.

If a dot product is positive, it means the two vectors point in generally the same direction.
If a dot product is negative, it means the two vectors point in generally opposite directions.
If a dot product is zero, it means the two vectors are perpendicular.
