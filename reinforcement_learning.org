#+StARTUP: entitiespretty
* Chapter 1 - The Reinforcement Learning Problem
** Exploration vs Exploitation
** The agent is only part of the system.
An agent engaged in reinforment learning might be a small part of a larger system. For example, monitoring the battery level in a robot, in this case the "robot" is the environment.
** Elements of Reinforcement Learning
*** Policy
A mapping from perceived state to action.
*** Reward signal
The immediate reward from the environment. An agents single goal is to maximize the reward received over the long run.
*** Value function
An estimate of the long term rewards a state or action may lead to. The agent seeks to maximize value, the long term reward it will receive.
*** Model
A model of the environment used for planning. Not all reinforcement learning methods use a model.
** Ideas
*** Partitioned Agents
How might you naturally or automatically break an agent down in to many smaller agents? Just like a neural network comprised of many simple neurons can do amazing things, many simple agents might be able to do amazing things which couldn't be acheived any other way.
*** Don't be afraid to hard code behaviors.
It's cool to imagine an agent learning from scratch, but it's probably more efficient to hard code some starting behaviors. Idealy, these would be soft behaviors that could be altered by the agent over time.
* Chapter 2 - Multi-arm Bandits
** Evaluation vs Instruction
In reinforcement learning the learning happens by evaluating the agents actions, rather than instructing the agent on what the correct action is. This is why exploration is necessary.
** Exploration vs Exploitation
With no exploration, the optimal action may never be found. Too little exploration causes the optimal solution to be found slowly. However, too much exploration, by its nature, takes suboptimal actions, so too much exploration can be wasteful.
** Way to balance Exploration and Exploitation
*** Ɛ-greedy
Explore a set percentage of the time, and exploit the rest of the time. The explore percentage may be decreased over time.
*** Optimistic initial values
By setting the initial value estimates high, the agent will try many action and be "disappointed" in all of them. But, it will be least disappointed the in best action. Thus, optimistic initial values can give us a natural way to encourage exploration. This method alone is not effective on non-stationary targets.
** Tracking expected value
- Average: Q_{n+1} = Q_n + 1/n (R_n - Q_n)
- Fixed learning rate: Q_{n+1} = Q_n + ⍶ (R_n - Q_n)
- Upper confidence bound: Q(a) + c * sqrt(log(t) / N(a))
  where c is a learning rate scalar
        N(a) is the number of times a has been selected in the past
        t is the current time step
- Gradient: e^(H(a)) / \sum e^(H(b))
  where a is what is selected
        b is the rest of the actions (the value on the bottom is a sum)
  Gradient update rule for H: H_{t+1} = H(a) + c (R - avg(R)) (1 - prob(a))
                              H_{t+1} = H(b) - c (R - avg(R)) prob(b)
** Ideas
*** Adjust agent hyperparameters using another agent.
*** Track uncertainty of actions and slowly increase the uncertainty over time.
If take an action and the result is unexpected, increase the uncertainty even further. You would probably have to assume the distribution shape to do this. E.g., you might assume the reward distributions are normal.
* Chapter 3 - Finite Markov Decision Processes
** The agent-environment interface
The agent selects actions based on a policy, and the environment responds with an updated state and a reward value.
** Be careful how you define the reward function
** RL tasks can be episodic or continuous
** What value function should be maximized?
Agents seek to maximize a value function. The value function is the expected reward over a time. A simple sum is a good value function for episodic tasks, but for continuous tasks a discounting factor is used to reduce the value of rewards far in the future, otherwise the value function would go to infinity in many cases. In such cases an agent might act slowly (because why hurry? it can be rewarded later) or have other problems.
** The Markov property / Markov states
If a state has the Markov property, then it is equally as valuable as having every detail of the entire state history. E.g., in Checkers the state of the board has the Markov property, because although there may have been many ways to reach the current state, none of that really matters, and the board itself has all the information needed to continue with the game.

States having the Markov property is an ideal to consider. However, success is still possible without the Markov property, and in some cases it is beneficial to throw out some information to help the algorithm focus on the more important information.
** Markov Decision Processes
A RL task that has the Markov property is a Markov decision process (MDP). A MDP can be defined as p(s\prime, r | s, a). In other words, a MDP can be defined as a table of probabilities from state/action pairs to state/reward pairs.
** Value Functions
v_\pi(s) is the expected value of s, if we follow policy \pi. It is called the state-value function for policy \pi.
q_\pi(s,a) is the expected value of action a from state s, if we follow policy \pi. It is called the action-value function for policy \pi.
The policy \pi is mapping from state to action probabilities.
v_\pi(s) = \sum_a \pi(a|s) \sum_{s',r} prob(s',r|s,a) (r + discount * v_\pi(s'))
The above equation is a Bellman equation, which is related to dynamic programming. Roughly speaking, the equation proves that a dynamic programming solution is possible.
** Optimal Policies
v_*(s) is basically the same as v_\pi(s) but assumes an optimal policy.
q_*(s,a) is basically the same as v_\pi(s,a) but assumes an optimal policy.
There is always a policy that is better than or equal to the current policy.
It is rarely possible to compute the optimal policy exactly, but there are ways of approximating it. The online nature of reinforcement learning allows effort to be spend on states that are likely to be encountered, while ignoring rare edge cases that would almost never be encountered. This sets RL appart from other approaches to solving MDPs.
** Review
Actions are the choices an agent can make, states are the bases for those choices, and rewards are the basis for evaluating choices.
There is one correct (optimal) value function for a given MDP. But there can be many optimal policies.
** Thoughts
*** Only useful in systems that need to adapt?
Is reinforcement learning only useful in systems that need to adapt to changing environments? In many of the use cases I have imagined a simple heuristic is both predictable and immediately effective.
* Chapter 4 - Dynamic Programming
Dynamic programming (DP) is of limited utility; it requires a perfect model of the environment. But it does provide a good theoretical foundation.
** Policy Evaluation
v_\pi(s) is guarenteed to converge if the discount factor is less than one, or if the policy will inevitably reach a terminal state. v_\pi(s) can be calculated with an iterative algorithm.
*** Calculating v_\pi(s) iteratively
v_\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a) (r + y * v_\pi(s'))
The formula gives the update rule. Because the formula is self referential, it means you can update the value of one state based on the value of other states. Every time this is done a state moves closer to it's true value. After many iterations the algorithm will converge.
** Policy Improvement
To improve a policy, for each state, choose the action that maximizes:
\sum_{s',r} prob(s',r|s,a) (r + y * v_\pi(s')) = \pi'(s)
Ties can be decided arbitrarily. Notice that this equation uses v_\pi(s) which can be calculated using Policy Evaluation (above).
** Policy Iteration
By iterating between policy evaluation and policy improvement (both described above) an optimal policy can be found. There are a variety of ways to perform this iteration with different performance tradeoffs.
** Terminology
Policy evaluation - find the values of all states for a specific policy
Policy improvement - improve a policy based on expected policy values
Policy iteration - combine policy evaluation and improvement iteratively
Value iteration - a specific method for policy iteration
** There is a lot of room in how to structure policy iteration.
As long as policy evaluation and improvement updates the values for each state, the algorithms will converge. Feel free to adjust the order of states, or even spend more time updating some states than others, etc.
** Bootstrapping
These dynamic programming methods update estimates (the value function is an estimate) based on estimates. This is called bootstrapping. This is obvious from the self referential definition of the functions.
** Thoughts
*** RL for autoscaling DynamoDB
RL would probably be good for autoscaling reinforcement learning. The reward function could take into account hosting cost, the business cost of throttles, etc.
*** RL for card games like Coup
RL would make a good Coup AI. Coup is a bluffing card game.
*** Things needed
Looking at the formulas I see three things that must be tracked:
- the policy, a function from state to action
- the transition probabilities from state/action pairs to state/reward pairs
- the value function which estimates how valuable a state will be under a certain policy
*** Transition probabilities, a case for supervised machine learning
Transition properties for very large spaces could use supervised machine learning algorithms, which are good for making predictions in large spaces and for states the have never seen before. There are probably other areas in reinforcement learning for supervised algorithms as well.
* Chapter 5 - Monte Carlo Methods
** Only applied to episodic tasks in this book
** Policy Evaluation
An obvious way to estimate the value function for a specific state is to simply average all returns observed after visiting that state. This idea of averaging returns underlies all Monte Carlo methods.
** Monte Carlo methods do not bootstrap
One implication of not bootstrapping is that you can start a simulation from any  state. You don't necissarily need to evaluate all states to get a good value estimate for a single state. You can just run many simulations starting at a single state, and end up with a good value estimate for that single state (and possibly some other states as well).
This leads to a trick called exploring starts, where all states (or state/action pairs) have a chance of being selected as the starting state for an episode, and thus all states will eventually have been visited an infinite number of times. Of course, this can only be done in a simulated environment.
*** Less harmed by violations of the Markov property
Because Monte Carlo methods do not update value extimates based on other value estimates (they do not bootstrap), they are harmed less by violations of the Markov property.
** q(s) (the action-value function) is especially useful in MC methods
*** Mantaining exploration with q(s)
To have an accurate q(s) function requires that all states and actions be visited at least several times, if not infinitely many times, and this does not normally happen when following a policy. Thus, exploration is necissary, and we have to balance exploration and exploitation.
** Off-Policy Learning
In off-policy learning, learning about a "target policy" which approximates the optimal policy is different than the "behavior policy" which determines the actions to take during learning.
This provides a more powerful framework in which to think about reinforcement learning. On-policy methods are just a special case of off-policy methods in which the target policy and behavior policy are the same.
*** Importance sampling
To estimate the target policy based on a behavior policy, observe the rewards from the behavior policy, and then scale them by p(\pi(a|s)) / p(\mu(a|s)) where \pi is the target policy and \mu is the behavior policy.
This means that if the behavior policy does something the target policy would never do, the weight will be low, and if the behavior policy does something that the target policy is very likely to do, the weight will be high.
*** 2 ways of averaging for the value function
Let P be the ratio p(\pi(a|s)) / p(\mu(a|s)).
Let G the the observed rewards after a visit to a state s.
Simple scale: \sum P*G / length(G)
Weighted importance sampling / weighted average: \sum P*G / \sum P
*** Coverage
The behavior policy must select all policies that the target policy might possibly pick over time. This is called coverage.
In other words, the behavior policy must have a non-zero probability of picking any of the actions that the target policy could possibly pick.
** Thoughts
*** Design states to be ML friendly
States should have a the markov property, and also be useful to any ML algorithms you plan to use.
*** First, get a little learning, then iterate
Keep thing simple and acheive a small amount of learning, then iterate. As you attempt to improve upon an algorithm it becomes harder and harder not to undo the learning you have already acheived.
* Chapter 6 - Temporal-Difference Learning
"If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning."
** TD methods are a combination of DP and MC methods
*** TD methods do not require a model of the environment, like MC methods
*** TD methods bootstrap, like DP methods
TD methods update estimates based, in part, on other estimattes. They bootstrap.
*** TD methods update estimates based on experience, like MC methods
** Comparison of how value functions are updated
*** MC updates the value function once the return is known at the end of the episode
V(S_t) <- V(S_t) + \alpha (G_t - V(S_t))
*** TD updates the value function immediately, after reaching the next state
V(S_t) <- V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))
where
\alpha - is a learning step rate
\gamma - is a discount factor in the range [0, 1)
**** TD error
The term (R_{t+1} + \gamma V(S_{t+1})) gives a more accurate estimate of the true value of S_t. It is subtracted from the previous estimate (V(S_t)) to give a difference which is used when updating V. This patten is called the TD error.
**** The state-action varient is very similar
Q(S_t, A_t) <- Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))
** TD method advantages
*** TD methods do not require a model of the environment
*** TD methods utilize immediate feedback
With MC methods, learning happens only at the end of an episode. TD methods are constantly learning after every single reward.
*** TD methods do not require episodic tasks
MC methods require that a task be broken into distinct episodes. TD methods can be used on both episodic and non-episodic tasks.
** Prediction and Control
In DP we have policy evaluation and policy improvement, and must iterate between these to reach the optimal policy.
In general, these can be though of as "prediction" and "control" respectively.
By iterating between improving our predictions based on experience, and controlling the actions we take, we can approach an optimal control policy.
*** Two problems:
- How will I improve my predictions based on experience?
- How will I control my actions such that I behave optimally, while also providing all the experiences needed to improve my predictions?
*** Dynamic programming, Monte Carlo, and temporal difference are variations on prediction
Issues of control are mostly the same with all three methods. The three methods are mainly variations on how to predict state/action values.
** Q-learning
Q(S_t, A_t) <- Q(S_t, A_t) + \alpha (R_{t+1} + \gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t))
*** Very similar to plain TD state-action update rule
The only difference between Q-learning and the TD state-action update rule is the `max_a` term.
I think this is because Q-learning is an off-policy method, and so it updates the state-action value based on the best possible choice rather than was was actually performed by the behavior policy.
*** Sometimes there isn't much difference between on-policy and off-policy methods
** Simplifying on/off-policy
The simple takeaway of the on/off-policy concept is to simply remember that the best policy you're aware of isn't necissarily the policy you have to follow when making decisions.
You don't have to be greedy.
** Sarsa (State Action Reward State Actions)
Q(S_t, A_t) <- Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))
*** Expected Sarsa
Expected Sarsa is a varient that uses the expected value in place of `Q(S_{t+1}, A_{t+1})`:
... + \gamma \sum_a \pi(a|S_{t+1}) Q(S_{t+1}, a) - ...
In words, a sum of the value of all possible actions, weighted by how likely the policy is to choose that action.
** Maximization Bias and Double Learning
V and Q are estimates. Like all estimates, they are rarely exactly right, and thus are either too low or too high.
Because Q-learning involves selecting the action that maximizes Q, there is significan "maximization bias", because the maximum value in Q is probably too high (which explains why it's the maximum).
*** Double Learning
One way to minimize the maximization bias is to use "double learning", which uses a twin Q function to estimate the maximum value. Half the time Q_1 is updated using estimates from Q_2, and the other half of the time vice-versa.
*** Double learning update rule:
Q_1(S_t, A_t) <- Q_1(S_t, A_t) + \alpha (R_{t+1} + \gamma Q_2(S_{t+1}, max_a Q_1(S_{t+1}, a)) - Q_1(S_t, A_t))
** Adapt the reinforcement learning framework as needed
Remember, reinforcement learning gives a framework as a guide, but there are special cases like:
*** Afterstates
Consider a turn based game. When a players turn begins, the game is in a certain state, the player then takes an action, and then the game is in an "afterstate" before the next player's turn begins.
This usually occurs when the dynamics of the environment are, at least, partially known.
Often it is more efficient to estimate the values of "afterstates" rather than distinguish between state and action as in the traditional reinforcement learning framework.
** TD methods outside of RL
TD methods have been used outside of reinforcement learning. They are a general way of making long term predictions about dynamical systems.
** Thoughts
*** Control hyperparameters with reinforcement learning - recusrive reinforcement learning
You could control \epsilon in \epsilon-greedy methods with a second reinforcement learning method.
* Chapter 7 - Multi-step Bootstrapping
Multi-step TD generalizes both MC and TD methods so that you can switch from one to the other smoothly.
They also decouple the action interval and the bootstrapping interval.
** TODO Continue reading chapter 7 from section 7.1
* Chapter 8 - Planning and Learning with Tabular Methods
** TODO Read all of chapter 8
* Chapter 9 - On-policy Prediction with Approximation
Many problems have state spaces that are too large for tabular methods. Here we can use function approximation.
** Bootstrapping machine learning models
A machine learning model might bootstrap and use it's own output to adjust it's own weights.
** Model limitations
A function approximation model is a compression (of sorts) of all states. This is most likely a lossy compression though. You probably cannot encode all state-value pairs 100% correctly. Some models will be better than others at encoding the state-value pairs.
** General stochastic gradient descent (SGD) method
\theta_{t+1} = \theta_t + \alpha * ( U_t - v(S_t, \theta_t) ) * \Delta{}v(S_t, \theta_t)

Where:
- \theta are parameters to a function approximation method
- \alpha is the step size parameter
- v(S_t, \theta_t) is the function approximation method
- U_t is an approximation to the actual long-term value of a state, it is the goal of v(S_t, \theta_t) to approximate U_t as closely as possible
- \Delta{}v(S_t, \theta_t) is the gradient descent, it is a vector which points the direction we must adjust \theta to reduce the error / difference between U_t and v(S_t, \theta_t)
** Bootstrapping bias
Bootstrapping can cause the function approximation model to hold onto biases. If the model is wrong about something, it will tend to continue being wrong about that thing because it is bootstrapping. Even so, bootstrapping often performs better in practice.
** Feature selection
As with other machine learning method, feature engineering is important. It is a good way to import knowledge about the problem at hand.

"Choosing features appropriate to the task is an important way of adding prior domain knowledge to reinforcement learning systems. Intuitively, the features should correspond to the natural features of the task, those along which generalization is most appropriate. If we are valuing states of a mobile robot, then we might want to have features for locations, degrees of remaining battery power, recent sonar readings, and so on."
** Feature combination techniques
Not all models can recognize interactions between different features. Here are some techniques to combine features to make their interactions visible to such models.
*** Polynomials
We can combine 2 states (s1, s2), with a polynomial which gives us (s1, s2, s1 * s2) or (s1, s2, s1 * s2, s1^2 * s2, s1 * s2^2, s1^2 * s2^2), etc.
*** Fourier Basis
"Another linear function approximation method is based on the time-honored Fourier series, which expresses periodic functions as a weighted sum of sine and cosine basis functions of different frequencies."
This can be helpful for periodic or repetitive tasks.
*** Coarse Coding / Tile Coding
Example: You could dividie up a 2D plane into several overlapping circles. The circles will become binary features to a model. If a point (a state) in the 2D plane is inside one of the circles, that feature will be 1, and otherwise 0, etc.
You can divide the space up in an orderly manner or randomly. It may help to have multiple "layers", each of which divides the state space in a different way, and each "layer" corresponds to several features.
*** Radial basis functions
These are similar to coarse coding or tile coding, but instead of binary features they use "soft edges" (like a bell curve). This adds complexity but produces a smooth feature space.
** Thought: Don't undervalue simple models
"Nonlinear methods may be able to fit target functions much more precisely, but the downside is greater computational complexity and, often, more manual tuning before learning is robust and efficient."
It's cool to use fancy machine learning models and marvel at what they can do, but it's also cool to deploy reinforcement learning agents that you know are robust and will respond to changes quickly and reliably.
* Chapter 10 - On-policy Control with Approximation
** TODO Read chapter 10
