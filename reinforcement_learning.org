* Chapter 1 - The Reinforcement Learning Problem
** Exploration vs Exploitation
** The agent is only part of the system.
An agent engaged in reinforment learning might be a small part of a larger system. For example, monitoring the battery level in a robot, in this case the "robot" is the environment.
** Elements of Reinforcement Learning
*** Policy
A mapping from perceived state to action.
*** Reward signal
The immediate reward from the environment. An agents single goal is to maximize the reward received over the long run.
*** Value function
An estimate of the long term rewards a state or action may lead to. The agent seeks to maximize value, the long term reward it will receive.
*** Model
A model of the environment used for planning. Not all reinforcement learning methods use a model.
** Ideas
*** Partitioned Agents
How might you naturally or automatically break an agent down in to many smaller agents? Just like a neural network comprised of many simple neurons can do amazing things, many simple agents might be able to do amazing things which couldn't be acheived any other way.
*** Don't be afraid to hard code behaviors.
It's cool to imagine an agent learning from scratch, but it's probably more efficient to hard code some starting behaviors. Idealy, these would be soft behaviors that could be altered by the agent over time.
* Chapter 2 - Multi-arm Bandits
** Evaluation vs Instruction
In reinforcement learning the learning happens by evaluating the agents actions, rather than instructing the agent on what the correct action is. This is why exploration is necessary.
** Exploration vs Exploitation
With no exploration, the optimal action may never be found. Too little exploration causes the optimal solution to be found slowly. However, too much exploration, by its nature, takes suboptimal actions, so too much exploration can be wasteful.
** Way to balance Exploration and Exploitation
*** Ɛ-greedy
Explore a set percentage of the time, and exploit the rest of the time. The explore percentage may be decreased over time.
*** Optimistic initial values
By setting the initial value estimates high, the agent will try many action and be "disappointed" in all of them. But, it will be least disappointed the in best action. Thus, optimistic initial values can give us a natural way to encourage exploration. This method alone is not effective on non-stationary targets.
** Tracking expected value
- Average: Q_{n+1} = Q_n + 1/n (R_n - Q_n)
- Fixed learning rate: Q_{n+1} = Q_n + ⍶ (R_n - Q_n)
- Upper confidence bound: Q(a) + c * sqrt(log(t) / N(a))
  where c is a learning rate scalar
        N(a) is the number of times a has been selected in the past
        t is the current time step
- Gradient: e^(H(a)) / \sum e^(H(b))
  where a is what is selected
        b is the rest of the actions (the value on the bottom is a sum)
  Gradient update rule for H: H_{t+1} = H(a) + c (R - avg(R)) (1 - prob(a))
                              H_{t+1} = H(b) - c (R - avg(R)) prob(b)
** Ideas
*** Adjust agent hyperparameters using another agent.
*** Track uncertainty of actions and slowly increase the uncertainty over time.
If take an action and the result is unexpected, increase the uncertainty even further. You would probably have to assume the distribution shape to do this. E.g., you might assume the reward distributions are normal.
* Chapter 3 - Finite Markov Decision Processes
** The agent-environment interface
The agent selects actions based on a policy, and the environment responds with an updated state and a reward value.
** Be careful how you define the reward function
** RL tasks can be episodic or continuous
** What value function should be maximized?
Agents seek to maximize a value function. The value function is the expected reward over a time. A simple sum is a good value function for episodic tasks, but for continuous tasks a discounting factor is used to reduce the value of rewards far in the future, otherwise the value function would go to infinity in many cases. In such cases an agent might act slowly (because why hurry? it can be rewarded later) or have other problems.
** The Markov property / Markov states
If a state has the Markov property, then it is equally as valuable as having every detail of the entire state history. E.g., in Checkers the state of the board has the Markov property, because although there may have been many ways to reach the current state, none of that really matters, and the board itself has all the information needed to continue with the game.

States having the Markov property is an ideal to consider. However, success is still possible without the Markov property, and in some cases it is beneficial to throw out some information to help the algorithm focus on the more important information.
** Markov Decision Processes
A RL task that has the Markov property is a Markov decision process (MDP). A MDP can be defined as p(s\prime, r | s, a). In other words, a MDP can be defined as a table of probabilities from state/action pairs to state/reward pairs.
** Value Functions
v_\pi(s) is the expected value of s, if we follow policy \pi. It is called the state-value function for policy \pi.
q_\pi(s,a) is the expected value of action a from state s, if we follow policy \pi. It is called the action-value function for policy \pi.
The policy \pi is mapping from state to action probabilities.
v_\pi(s) = \sum_a \pi(a|s) \sum_{s',r} prob(s',r|s,a) (r + discount * v_\pi(s'))
The above equation is a Bellman equation, which is related to dynamic programming. Roughly speaking, the equation proves that a dynamic programming solution is possible.
** Optimal Policies
v_*(s) is basically the same as v_\pi(s) but assumes an optimal policy.
q_*(s,a) is basically the same as v_\pi(s,a) but assumes an optimal policy.
There is always a policy that is better than or equal to the current policy.
It is rarely possible to compute the optimal policy exactly, but there are ways of approximating it. The online nature of reinforcement learning allows effort to be spend on states that are likely to be encountered, while ignoring rare edge cases that would almost never be encountered. This sets RL appart from other approaches to solving MDPs.
** Review
Actions are the choices an agent can make, states are the bases for those choices, and rewards are the basis for evaluating choices.
There is one correct (optimal) value function for a given MDP. But there can be many optimal policies.
** Thoughts
*** Only useful in systems that need to adapt?
Is reinforcement learning only useful in systems that need to adapt to changing environments? In many of the use cases I have imagined a simple heuristic is both predictable and immediately effective.
* Chapter 4 - Dynamic Programming
Dynamic programming (DP) is of limited utility; it requires a perfect model of the environment. But it does provide a good theoretical foundation.
** Policy Evaluation
v_\pi(s) is guarenteed to converge if the discount factor is less than one, or if the policy will inevitably reach a terminal state. v_\pi(s) can be calculated with an iterative algorithm.
*** Calculating v_\pi(s) iteratively
v_\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a) (r + y * v_\pi(s'))
The formula gives the update rule. Because the formula is self referential, it means you can update the value of one state based on the value of other states. Every time this is done a state moves closer to it's true value. After many iterations the algorithm will converge.
** Policy Improvement
To improve a policy, for each state, choose the action that maximizes:
\sum_{s',r} prob(s',r|s,a) (r + y * v_\pi(s')) = \pi'(s)
Ties can be decided arbitrarily. Notice that this equation uses v_\pi(s) which can be calculated using Policy Evaluation (above).
** Policy Iteration
By iterating between policy evaluation and policy improvement (both described above) an optimal policy can be found. There are a variety of ways to perform this iteration with different performance tradeoffs.
** Terminology
Policy evaluation - find the values of all states for a specific policy
Policy improvement - improve a policy based on expected policy values
Policy iteration - combine policy evaluation and improvement iteratively
Value iteration - a specific method for policy iteration
** There is a lot of room in how to structure policy iteration.
As long as policy evaluation and improvement updates the values for each state, the algorithms will converge. Feel free to adjust the order of states, or even spend more time updating some states than others, etc.
** Bootstrapping
These dynamic programming methods update estimates (the value function is an estimate) based on estimates. This is called bootstrapping. This is obvious from the self referential definition of the functions.
** Thoughts
*** RL for autoscaling DynamoDB
RL would probably be good for autoscaling reinforcement learning. The reward function could take into account hosting cost, the business cost of throttles, etc.
*** RL for card games like Coup
RL would make a good Coup AI. Coup is a bluffing card game.
*** Things needed
Looking at the formulas I see three things that must be tracked:
- the policy, a function from state to action
- the transition probabilities from state/action pairs to state/reward pairs
- the value function which estimates how valuable a state will be under a certain policy
*** Transition probabilities, a case for supervised machine learning
Transition properties for very large spaces could use supervised machine learning algorithms, which are good for making predictions in large spaces and for states the have never seen before. There are probably other areas in reinforcement learning for supervised algorithms as well.
* Chapter 5 - Monte Carlo Methods
** Only applied to episodic tasks in this book
** Policy Evaluation
An obvious way to estimate the value function for a specific state is to simply average all returns observed after visiting that state. This idea of averaging returns underlies all Monte Carlo methods.
** Monte Carlo methods do not bootstrap
One implication of not bootstrapping is that you can start a simulation from any  state. You don't necissarily need to evaluate all states to get a good value estimate for a single state. You can just run many simulations starting at a single state, and end up with a good value estimate for that single state (and possibly some other states as well).
This leads to a trick called exploring starts, where all states (or state/action pairs) have a chance of being selected as the starting state for an episode, and thus all states will eventually have been visited an infinite number of times. Of course, this can only be done in a simulated environment.
*** Less harmed by violations of the Markov property
Because Monte Carlo methods do not update value extimates based on other value estimates (they do not bootstrap), they are harmed less by violations of the Markov property.
** q(s) (the action-value function) is especially useful in MC methods
*** Mantaining exploration with q(s)
To have an accurate q(s) function requires that all states and actions be visited at least several times, if not infinitely many times, and this does not normally happen when following a policy. Thus, exploration is necissary, and we have to balance exploration and exploitation.
** Off-Policy Learning
In off-policy learning, learning about a "target policy" which approximates the optimal policy is different than the "behavior policy" which determines the actions to take during learning.
This provides a more powerful framework in which to think about reinforcement learning. On-policy methods are just a special case of off-policy methods in which the target policy and behavior policy are the same.
*** Importance sampling
To estimate the target policy based on a behavior policy, observe the rewards from the behavior policy, and then scale them by p(\pi(a|s)) / p(\mu(a|s)) where \pi is the target policy and \mu is the behavior policy.
This means that if the behavior policy does something the target policy would never do, the weight will be low, and if the behavior policy does something that the target policy is very likely to do, the weight will be high.
*** 2 ways of averaging for the value function
Let P be the ratio p(\pi(a|s)) / p(\mu(a|s)).
Let G the the observed rewards after a visit to a state s.
Simple scale: \sum P*G / length(G)
Weighted importance sampling / weighted average: \sum P*G / \sum P
*** Coverage
The behavior policy must select all policies that the target policy might possibly pick over time. This is called coverage.
In other words, the behavior policy must have a non-zero probability of picking any of the actions that the target policy could possibly pick. 
** Thoughts
*** Design states to be ML friendly
States should have a the markov property, and also be useful to any ML algorithms you plan to use.
*** First, get a little learning, then iterate
Keep thing simple and acheive a small amount of learning, then iterate. As you attempt to improve upon an algorithm it becomes harder and harder not to undo the learning you have already acheived.
